table(forcasted, teData$Species)
windows()
plot(citreeResult2)
# 유클리디안 거리
# 1. 유클리디안 거리 계산법
# 1-1 matrix 객체 생성
rm(list = ls())
library(party)
# 2-1 데이터 가져오기 샘플링
data(iris)
str(iris)
set.seed(1000)
iris_sample <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7,0.3))
iris_sample
# 2-2 분류모델 생성
iris_train <- iris[iris_sample == 1, ]
head(iris_train)
iris_test <- iris[iris_sample == 2, ]
head(iris_test)
formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
# 2-3 테스트 데이터를 이용하여 분류
ctree_result <- ctree(formula, data = iris_test)
ctree_result2 <- predict(ctree_result, data = iris_test)
ctree_result2
iris_test$Species
# 예측결과와 실제값 비교
table(ctree_result2, iris_test$Species)
windows()
plot(ctree_result2)
windows()
plot(ctree_result)
library(rpart)
library(rpart.plot)
# 빅데이터 A조 이순규
# 1. iris데이터를 이용 CART기법 적용 rpart()함수 이용 분류분석
rm(list = ls())
library(caret)
# 1-2 분류모델 생성
iris_idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train <- iris[iris.idx, ]
iris_train <- iris[iris_idx, ]
iris_test <- iris[-iris_idx, ]
iris_model <- rpart(Species ~ ., data = iris_train)
summary(iris.model)
summary(iris_model)
# 1-3 테스트 데이터를 이용하여 분류
iris_pred <- predict(iris_model, iris_test, type = 'class')
iris_pred
# 1-4 예측 정확도
iris_result <- confusionMatrix(iris_pred, iris_test$Species)
iris_result
# 2. iris데이터를 이용 조건부 추론나무 cpart()함수 이용 분류분석
rm(list = ls())
library(party)
# 2-1 데이터 가져오기 샘플링
data(iris)
iris_idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train <- iris[iris_idx, ]
iris_test <- iris[-iris_idx, ]
# 2-2 분류모델 생성
iris_train <- iris[iris_idx == 1, ]
head(iris_train)
iris_test <- iris[iris_idx == 2, ]
head(iris_test)
formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
ctree_result <- ctree(formula, data = iris_test)
# 2-3 테스트 데이터를 이용하여 분류
ctree_result2 <- predict(ctree_result, data = iris_test)
ctree_result2
iris_test$Species
# 예측결과와 실제값 비교
table(ctree_result2, iris_test$Species)
windows()
plot(ctree_result)
# 2. iris데이터를 이용 조건부 추론나무 cpart()함수 이용 분류분석
rm(list = ls())
# 2-1 데이터 가져오기 샘플링
data(iris)
iris_idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train <- iris[iris_idx, ]
iris_test <- iris[-iris_idx, ]
# 2-2 분류모델 생성
formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
ctree_result <- ctree(formula, data = iris_train)
ctree_result
# 2-3 테스트 데이터를 이용하여 분류
ctree_result2 <- predict(ctree_result, data = iris_test)
ctree_result2
iris_test$Species
# 예측결과와 실제값 비교
table(ctree_result2, iris_test$Species)
# 2. iris데이터를 이용 조건부 추론나무 cpart()함수 이용 분류분석
rm(list = ls())
iris_idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train <- iris[iris_idx, ]
iris_test <- iris[-iris_idx, ]
# 2-2 분류모델 생성
iris_tree <- ctree(Species ~ ., data = iris_train)
iris_tree
# 2-3 테스트 데이터를 이용하여 분류
iris_pred <- predict(iris_tree, iris_test)
table(iris_pred, iris_test$Species)
windows()
par(mfrow = c(1,2))
plot(iris_tree)  # 의사결정트리 플로팅
plot(iris_pred)  # 분류 결과 플로팅
plot(iris_tree)  # 의사결정트리 플로팅
plot(iris_pred)  # 분류 결과 플로팅
windows()
plot(iris_tree)  # 의사결정트리 플로팅
windows()
plot(iris_pred)  # 분류 결과 플로팅
library(cluster)
data(iris)
# 3-1 유클리드 거리계산
iris_df <- iris[c(1:4)]
soongyu <- dist(iris_df)
head(soongyu)
# 3. iris 데이터셋의 1~4번 변수를 대상으로 유클리드 거리 매트릭스를 구하여 idist 저장한 뒤
# 계층적 클러스터링을 적용하여 결과를 시각화
rm(list = ls())
library(cluster)
data(iris)
# 3-1 유클리드 거리계산
iris_df <- iris[c(1:4)]
soongyu <- dist(iris_df)
head(soongyu)
# 3. iris 데이터셋의 1~4번 변수를 대상으로 유클리드 거리 매트릭스를 구하여 idist 저장한 뒤
# 계층적 클러스터링을 적용하여 결과를 시각화
rm(list = ls())
library(cluster)
data(iris)
# 3-1 유클리드 거리계산
iris_df <- iris[1:4]
soongyu <- dist(iris_df)
head(soongyu)
# 3. iris 데이터셋의 1~4번 변수를 대상으로 유클리드 거리 매트릭스를 구하여 idist 저장한 뒤
# 계층적 클러스터링을 적용하여 결과를 시각화
rm(list = ls())
library(cluster)
data(iris)
# 3-1 유클리드 거리계산
soongyu <- dist(iris[1:4])
head(soongyu)
soongyu
# 3-1 유클리드 거리계산
iris_df <- iris[c(1:4)]
soongyu <- dist(iris_df)
soongyu
# 3-2 계층형 군집 분석(클러스터링)
iris_hc <- hclust(soongyu)
iris_hc
windows()
plot(iris_hc, hang = -1)
windows()
plot(iris_hc, hang = -1)
windows()
iris_hclust(iris_hc, k = 3, border ="red")
windows()
rect.hclust(iris_hc, k = 3, border ="red")
# 3-4 그룹 수를 3개로 지정, 그룹별로 테두리 표시
rect.hclust(iris_hc, k = 3, border ="red")
# 3. iris 데이터셋의 1~4번 변수를 대상으로 유클리드 거리 매트릭스를 구하여 idist 저장한 뒤
# 계층적 클러스터링을 적용하여 결과를 시각화
rm(list = ls())
library(cluster)
data(iris)
# 3-1 유클리드 거리계산
iris_df <- iris[, c(1:4)]
idist <- dist(iris_df, method = 'euclidean')
# 3-2 계층형 군집 분석(클러스터링)
iris_hc <- hclust(soongyu)
iris_hc
# 3-2 계층형 군집 분석(클러스터링)
iris_hc <- hclust(idist)
iris_hc
windows()
plot(iris_hc, hang = -1)
# 3-4 그룹 수를 3개로 지정, 그룹별로 테두리 표시
rect.hclust(iris_hc, k = 3, border ="red")
head(iris_df)
# 3. iris 데이터셋의 1~4번 변수를 대상으로 유클리드 거리 매트릭스를 구하여 idist 저장한 뒤
# 계층적 클러스터링을 적용하여 결과를 시각화
rm(list = ls())
library(cluster)
data(iris)
iris2 <- iris[, c(1:4)]
head(iris2)
idist <- dist(iris2, method = 'euclidean')
# (2) 계층적 군집 분석(클러스터링)
hc <- hclust(idist)
# (3) 분류결과를 대상으로 음수값을 제거하여 덴드로그램 시각화
plot(hc, hang = -1)
# (4) 그룹수를 3개로 지정하여 그룹별로 테두리 표시
rect.hclust(hc, k = 3, border = 'blueviolet')
# 3. iris 데이터셋의 1~4번 변수를 대상으로 유클리드 거리 매트릭스를 구하여 idist 저장한 뒤
# 계층적 클러스터링을 적용하여 결과를 시각화
rm(list = ls())
library(cluster)
data(iris)
# 3-1 유클리드 거리계산
iris_df <- iris[, c(1:4)]
head(iris_df)
idist <- dist(iris_df, method = 'euclidean')
# 3-2 계층형 군집 분석(클러스터링)
iris_hc <- hclust(idist)
# 3-3 분류결과를 대상으로 음수값을 제거 덴드로그램 시각화
plot(iris_hc, hang = -1)
# 3-4 그룹 수를 3개로 지정, 그룹별로 테두리 표시
rect.hclust(iris_hc, k = 3, border ="red")
# 빅데이터 A조 이순규
# 1. iris데이터를 이용 CART기법 적용 rpart()함수 이용 분류분석
rm(list = ls())
library(rpart)
library(caret)
# 1-1 데이터 가져오기 샘플링
data(iris)
iris_idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train <- iris[iris_idx, ]
iris_test <- iris[-iris_idx, ]
# 1-2 분류모델 생성
iris_model <- rpart(Species ~ ., data = iris_train)
summary(iris_model)
# 1-3 테스트 데이터를 이용하여 분류
iris_pred <- predict(iris_model, iris_test, type = 'class')
iris_pred
# 1-4 예측 정확도
iris_result <- confusionMatrix(iris_pred, iris_test$Species)
iris_result
# 2. iris데이터를 이용 조건부 추론나무 ctree()함수 이용 분류분석
rm(list = ls())
library(party)
# 2. iris데이터를 이용 조건부 추론나무 ctree()함수 이용 분류분석
rm(list = ls())
library(party)
# 2-1 데이터 가져오기 샘플링
data(iris)
iris_idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train <- iris[iris_idx, ]
iris_test <- iris[-iris_idx, ]
# 2-2 분류모델 생성
iris_tree <- ctree(Species ~ ., data = iris_train)
iris_tree
# 2-3 테스트 데이터를 이용하여 분류
iris_pred <- predict(iris_tree, iris_test)
table(iris_pred, iris_test$Species)
windows()
plot(iris_tree)  # 의사결정트리 시각화
windows()
plot(iris_pred)  # 분류 결과 시각화
# 3. iris 데이터셋의 1~4번 변수를 대상으로 유클리드 거리 매트릭스를 구하여 idist 저장한 뒤
# 계층적 클러스터링을 적용하여 결과를 시각화
rm(list = ls())
library(cluster)
data(iris)
# 3-1 유클리드 거리계산
iris_df <- iris[, c(1:4)]
head(iris_df)
idist <- dist(iris_df, method = 'euclidean')
# 3-2 계층형 군집 분석(클러스터링)
iris_hc <- hclust(idist)
# 3-3 분류결과를 대상으로 음수값을 제거 덴드로그램 시각화
plot(iris_hc, hang = -1)
# 3-4 그룹 수를 3개로 지정, 그룹별로 테두리 표시
rect.hclust(iris_hc, k = 3, border ="red")
# 빅데이터 A조 이순규
# 1. iris데이터를 이용 CART기법 적용 rpart()함수 이용 분류분석
rm(list = ls())
library(rpart)
library(caret)
# 1-1 데이터 가져오기 샘플링
data(iris)
set.seed(1000)
iris_idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train <- iris[iris_idx, ]
iris_test <- iris[-iris_idx, ]
# 1-2 분류모델 생성
iris_model <- rpart(Species ~ ., data = iris_train)
summary(iris_model)
# 1-3 테스트 데이터를 이용하여 분류
iris_pred <- predict(iris_model, iris_test, type = 'class')
iris_pred
# 1-4 예측 정확도
iris_result <- confusionMatrix(iris_pred, iris_test$Species)
iris_result
# 2. iris데이터를 이용 조건부 추론나무 ctree()함수 이용 분류분석
rm(list = ls())
library(party)
# 2-1 데이터 가져오기 샘플링
data(iris)
set.seed(1000)
iris_idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train <- iris[iris_idx, ]
iris_test <- iris[-iris_idx, ]
# 2-2 분류모델 생성
iris_tree <- ctree(Species ~ ., data = iris_train)
iris_tree
# 2-3 테스트 데이터를 이용하여 분류
iris_pred <- predict(iris_tree, iris_test)
table(iris_pred, iris_test$Species)
windows()
plot(iris_tree)  # 의사결정트리 시각화
windows()
plot(iris_pred)  # 분류 결과 시각화
# 유클리디안 거리
# 1. 유클리디안 거리 계산법
# 1-1 matrix 객체 생성
rm(list = ls())
# 3. 신입사원의 면접시험 결과를 군집 분석
getwd()
setwd('E:\BigDate\R programing\dataset4\dataset4')
setwd('E:\BigDate\R programing\dataset4\dataset4')
setwd('E:/BigDate/R programing/dataset4/dataset4')
interview <- read.csv('interview.csv', header = TRUE)
names(interview)
head(interview)
# 3-1 유클리디안 거리계산
interview_df <- interview[c(2:7)]
idist <- dist(interview_df)
head(interview_df)
# 3-2 계층적 군집 분석
hc <- hclust(idist)
hc
# 3-3 군집분석 시각화
plot(hc, hang = -1)
# 3-4 군집단위 테두리 생성
rect.hclust(hc, k = 3, border = 'red')
# 4. 군집별 특징 보기
# 4-1 군집별 서브 셋 만들기
g1 <- subset(interview, no == 108 | no == 110 | no == 107 | no == 112 | no == 115)
g2 <- subset(interview, no == 102 | no == 101 | no == 104 | no == 106 | no == 113)
g3 <- subset(interview, no == 105 | no == 114 | no == 109 | no == 103 | no == 111)
# 4-2 각 서브 셋의 요약통계량 보기
summary(g1)
summary(g2)
summary(g3)
# 5. 군집 수 자르기
# 5) iris 데이터셋을 대상으로 군집 수 자르기
# 5-1 유클리디안 거리 계산
idist <- dist(iris[1:4])
hc <- hclust(idist)
plot(hc, hang = -1)
# 5-2 군집 수 자르기
ghc <- cutree(hc, k = 3)
ghc
# 5-3 iris데이터 셋에 ghc 컬럼 추가
iris$ghc <- ghc
table(iris$ghc)
head(iris)
# 5-4 요약통계량 구하기
g1 <- subset(iris, ghc == 1)
summary(g1[1:4])
g2 <- subset(iris, ghc == 2)
summary(g2[1:4])
g3 <- subset(iris, ghc == 3)
summary(g3[1:4])
# 6. 비게층적 군집분석
# k-means알고리즘에 군집 수를 적용하여 군집별로 시각화
# 6-1 군집 분석에 사용할 변수 추출
library(ggplot2)
data(diamonds)
t <- sample(1:nrow(diamonds), 1000)
test <- diamonds[t, ]
dim(test)
head(test)
mydia <- test[c('price', 'carat', 'depth', 'table')]
head(mydia)
# 6-2 계층적 군집 분석(탐색적 분석)
result <- hclust(dist(mydia), method = 'average')
result
plot(result, hang = -1)
# 6-3 비계층적 군집 분석
result2 <- kmeans(mydia, 3)
names(result2)
result2$cluster
mydia$cluster <- result2$cluster
head(mydia)
# 6-4 변수 간의 상관계수 보기
cor(mydia[, -5], method = 'person')
# 6-4 변수 간의 상관계수 보기
cor(mydia[, -5], method = 'pearson')
plot(mydia[, -5])
# 6-5 상관계수를 색상으로 시각화
install.packages('mclust')
library(mclust)
library(corrgram)
corrgram(mydia[, -5], upper.panel = panel.conf)
corrgram(mydia[, -5], lower.panel = panel.conf)
# 6-6 비계층적 군집 시각화
plot(mydia$carat, mydia$price, col = mydia$cluster)
points(result2$centers[, c('carat', 'price')], col = c(3, 1, 2), pch = 8, cex = 5)
# 7. 계층적 군집법
# 7-1 iris 데이터셋 로딩(50개)
rm(list = ls())
idx <- sample(1:dim(iris)[1], 50)
idx
irisSample <- iris[idx, ]
head(irisSample)
# 7-2 species 데이터 제거
irisSample$Species <- NULL
head(irisSample)
# 7-3 계층적 군집법 실행
hc_result <- hclust(dist(irisSample), method = 'ave')
hc_result
# 7-4 군집 결과 시각화
plot(hc_result, hang = -1, labels = iris$Species[idx])
rect.hclust(hc_result, k = 4)
# 8. k 평균 군집법
# 8-1 iris 데이터셋 로딩
rm(list = ls())
data(iris)
iris
iris2 <- iris
head(iris2)
# 8-3 k-means clustering 실행
kmeans_result <- kmeans(iris2, 6)
kmeans_result
# 8-3 k-means clustering 실행
kmeans_result <- kmeans(iris2, 6)
# 8-3 k-means clustering 실행
kmeans_result <- kmeans(iris2, 5)
# 8-3 k-means clustering 실행
kmeans_result <- kmeans(iris2, 3)
# 8. k 평균 군집법
# 8-1 iris 데이터셋 로딩
rm(list = ls())
data(iris)
iris
iris2 <- iris
# 8-2 species 데이터 제거
iris2$Species <- NULL
head(iris2)
# 8-3 k-means clustering 실행
kmeans_result <- kmeans(iris2, 6)
kmeans_result
str(kmeans_result)
# 8-4 군집 결과 시각화
plot(iris2[c("Sepal.Length", "Sepal.Width")], col=kmeans_result$cluster)
points(kmeans_result$centers[, c("Sepal.Length", "Sepal.Width")], col=1:4, pch=8, cex=2)
plot(iris2[c("Petal.Length", "Petal.Width")], col=kmeans_result$cluster)
points(kmeans_result$centers[, c("Petal.Length", "Petal.Width")], col=1:4, pch=8, cex=2)
# 8-5 군집의 수 결정
kmeans_result <- kmeans(iris2, 7)
plot(iris2[c("Sepal.Length", "Sepal.Width")], col=kmeans_result$cluster)
points(kmeans_result$centers[, c("Sepal.Length", "Sepal.Width")], col=1:4, pch=8, cex=2)
# 9. k 중심 군집법
# 9-1 패키지 설치
install.packages("fpc")
library(fpc)
# 9-2 군집의 수 설정. 임의로 6개
pamk_result <- pamk(iris2, 5)
pamk_result
# 9-3 군집의 수 확인
pamk_result$nc
table(pamk_result$pamobject$clustering, iris$Species)
layout(matrix(c(1,2),1,2))
plot(pamk_result$pamobject)
# 10-2 species 컬럼 제거
iris2 <- iris[-5]
head(iris2)
# 10-3 밀도기반 군집
db_result <- dbscan(iris2, eps=0.42, MinPts=5)
db_result
# 10-4시각화
plot(db_result, iris2)
#자세히 보기
plot(db_result, iris2[c(1,4)])
# 10-5 자세히 보기
plot(db_result, iris2[c(1,4)])
# 10-6 군집 결과 보기
plotcluster(iris2, db_result$cluster)
# 텍스트 마이닝
install.packages('tm')
library(tm)
getwd()
inputText <- readLines('aaa.txt')
inputText <- readLines('E:/aaa.txt')
inputText <- readLines('E:/aaa.txt')
inputText <- readline('E:/aaa.txt')
print(inputText)
inputText <- readline('E:/aaa.txt')
??
??
???
????
exit
inputText <- readLines('E:/aaa.txt')
inputText <- readLines('E:/input.txt')
inputText <- readLines('E:/input.txt')
print(inputText)
testText <- VectorSource(inputText)
corpus <- Corpus(testText)
corpus <- tm_map(corpus, removePunctuation)
# 유클리디안 거리
# 1. 유클리디안 거리 계산법
# 1-1 matrix 객체 생성
rm(list = ls())
library(tm)
inputText <- readLines('E:/input.txt')
print(inputText)
testText <- VectorSource(inputText)
corpus <- Corpus(testText)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c('0', '1', '2', '3', '4', '5', '6', '7', '8', '9'))
testText
# 텍스트 마이닝
rm(list = ls())
library(tm)
inputText <- readLines('E:/input.txt')
print(inputText)
myCorpus <-Corpus(VectorSource(inputText))
soon <- tm_map(myCorpus, removePunctuation)
